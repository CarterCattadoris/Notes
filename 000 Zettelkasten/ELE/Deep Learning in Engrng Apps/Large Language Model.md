202602101512
Status: #idea
Tags: [[Deep Learning (ELE400)]], [[Transformer]]

## Content

A **Large Language Model (LLM)** is a neural network with billions of parameters trained on massive text datasets to understand and generate human language.

### Enabled by Transformers

The [[Transformer]] architecture's parallel processing enabled training of LLMs by significantly decreasing training times for long sequences.

### Examples

- **GPT** (Generative Pre-trained Transformer) - Autoregressive language model
- **BERT** (Bidirectional Encoder Representations from Transformers) - Bidirectional understanding

### Characteristics

- Billions of parameters
- Capture wide range of human language and knowledge
- Learn complex language representations
- Can perform various language tasks (translation, summarization, Q&A)

### Impact

- Pushing research into more generalizable AI systems
- Foundation for modern conversational AI
- Enable transfer learning across language tasks
