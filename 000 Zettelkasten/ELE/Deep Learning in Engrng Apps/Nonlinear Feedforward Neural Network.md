202601281742
Status: #idea
Tags: [[Deep Learning (ELE400)]]

[[Feedforward Neural Network]] containing nonlinear [[Activation Functions]]

### Characteristics
- At least one layer uses nonlinear activation (sigmoid, ReLU, etc.)
- Can learn complex, non-linear relationships
- Hidden layers provide representational power

### Advantages over Linear Networks
- Can solve non-linearly separable problems (e.g., XOR)
- Universal approximation theorem applies
- Depth adds computational power

### Common Architectures
- Multi-layer perceptron (MLP)
- Deep neural networks
- Convolutional neural networks

### Training
Requires [[Backpropagation]] due to hidden layer complexity

---
### References
